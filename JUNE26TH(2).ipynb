{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyN41uRnIqKvlCz1+Ti8skmb",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starlingomez/preworkmac/blob/master/JUNE26TH(2).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uN89pblhX_hi"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "GRAYSON RODRIGUEZ"
      ],
      "metadata": {
        "id": "1o_4kbpUYD0Y"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Organize the data into a DataFrame\n",
        "data = {\n",
        "    'innings_pitched': [1.1, 4.1, 4.2, 5, 5, 5, 5.2, 5.2, 6, 6, 6, 6, 6.1, 6.1, 8],\n",
        "    'total_batter_faced': [15, 20, 18, 23, 24, 23, 23, 23, 21, 24, 22, 23, 22, 22, 28],\n",
        "    'strikeouts_per_nine_inning': [6.75, 16.62, 11.57, 10.8, 10.8, 10.8, 7.94, 14.29, 9, 21, 4.5, 7.5, 11.37, 7.11, 10.13],\n",
        "    'strikeouts': [1, 8, 6, 6, 6, 6, 5, 9, 6, 14, 3, 5, 8, 5, 9]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the independent variables and the dependent variable\n",
        "X = df[['innings_pitched', 'total_batter_faced', 'strikeouts_per_nine']].values\n",
        "y = df['strikeouts'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for the neural network\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with a modified learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the neural network model\n",
        "model.fit(X_train_scaled, y_train, epochs=200, batch_size=4, verbose=1)\n",
        "\n",
        "# Predict on the test set with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate performance metrics for the neural network\n",
        "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
        "r2_nn = r2_score(y_test, y_pred_nn)\n",
        "\n",
        "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n",
        "print(f\"Neural Network R-squared: {r2_nn}\")\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.7\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgboost_model = xgb.train(params, dtrain, num_boost_round=200)\n",
        "\n",
        "# Predict on the test set with XGBoost\n",
        "y_pred_xgb = xgboost_model.predict(dtest)\n",
        "\n",
        "# Calculate performance metrics for XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb}\")\n",
        "print(f\"XGBoost R-squared: {r2_xgb}\")\n",
        "\n",
        "# Output the first model prediction\n",
        "print(\"Neural Network Predicted strikeout:\", y_pred_nn.flatten()[0])\n",
        "print(\"XGBoost Predicted strikeout:\", y_pred_xgb[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 350
        },
        "id": "PQhRsfutYz3j",
        "outputId": "e77e500e-bd0e-4787-ca67-438b7b0f5619"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "error",
          "ename": "KeyError",
          "evalue": "\"['strikeouts_per_nine'] not in index\"",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-4-526c8b74b8cf>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[0;31m# Define the independent variables and the dependent variable\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m \u001b[0mX\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'innings_pitched'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'total_batter_faced'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'strikeouts_per_nine'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     22\u001b[0m \u001b[0my\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'strikeouts'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3765\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3766\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3767\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcolumns\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_indexer_strict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m\"columns\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3768\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3769\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_get_indexer_strict\u001b[0;34m(self, key, axis_name)\u001b[0m\n\u001b[1;32m   5875\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5876\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5877\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_raise_if_missing\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis_name\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5878\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5879\u001b[0m         \u001b[0mkeyarr\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtake\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindexer\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/pandas/core/indexes/base.py\u001b[0m in \u001b[0;36m_raise_if_missing\u001b[0;34m(self, key, indexer, axis_name)\u001b[0m\n\u001b[1;32m   5939\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5940\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 5941\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   5942\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   5943\u001b[0m     \u001b[0;34m@\u001b[0m\u001b[0moverload\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyError\u001b[0m: \"['strikeouts_per_nine'] not in index\""
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "LUIS GIL"
      ],
      "metadata": {
        "id": "U-6zFATRZuOg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Organize the data into a DataFrame\n",
        "data = {\n",
        "    'innings_pitched': [1.1, 4.1, 4.2, 5, 5, 5, 5.2, 5.2, 6, 6, 6, 6, 6.1, 6.1, 8],\n",
        "    'total_batter_faced': [15, 20, 18, 23, 24, 23, 23, 23, 21, 24, 22, 23, 22, 22, 28],\n",
        "    'strikeouts_per_nine_inning': [6.75, 16.62, 11.57, 10.8, 10.8, 10.8, 7.94, 14.29, 9, 21, 4.5, 7.5, 11.37, 7.11, 10.13],\n",
        "    'strikeouts': [1, 8, 6, 6, 6, 6, 5, 9, 6, 14, 3, 5, 8, 5, 9]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the independent variables and the dependent variable\n",
        "X = df[['innings_pitched', 'total_batter_faced', 'strikeouts_per_nine_inning']].values\n",
        "y = df['strikeouts'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for the neural network\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with a modified learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the neural network model\n",
        "model.fit(X_train_scaled, y_train, epochs=200, batch_size=4, verbose=1)\n",
        "\n",
        "# Predict on the test set with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate performance metrics for the neural network\n",
        "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
        "r2_nn = r2_score(y_test, y_pred_nn)\n",
        "\n",
        "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n",
        "print(f\"Neural Network R-squared: {r2_nn}\")\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.7\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgboost_model = xgb.train(params, dtrain, num_boost_round=200)\n",
        "\n",
        "# Predict on the test set with XGBoost\n",
        "y_pred_xgb = xgboost_model.predict(dtest)\n",
        "\n",
        "# Calculate performance metrics for XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb}\")\n",
        "print(f\"XGBoost R-squared: {r2_xgb}\")\n",
        "\n",
        "# Output the first model prediction\n",
        "print(\"Neural Network Predicted strikeout:\", y_pred_nn.flatten()[0])\n",
        "print(\"XGBoost Predicted strikeout:\", y_pred_xgb[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "CSmKo-U6acm5",
        "outputId": "0578ed5b-5cba-408c-f9aa-fe3c69ea7401"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "3/3 [==============================] - 1s 7ms/step - loss: 46.2273\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 45.3867\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 43.4538\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 41.8941\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 41.0932\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 40.0417\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 37.4615\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 37.4322\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 35.0011\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 33.3482\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 32.2207\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 28.9531\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 27.1500\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 25.2976\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 23.7619\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 19.4216\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 17.9526\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 15.9815\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 13.3722\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 12.2816\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 10.9665\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 8.9193\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 5.4817\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 6.3776\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 5.2852\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 6.8095\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 9.4192\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 4.6748\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 5.5098\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 5.3998\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 5.4979\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 5.5656\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 4.3170\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 5.2481\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 3.5979\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 4.1163\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 6.8504\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 4.6522\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 3.5411\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 2.5051\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 3.2477\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.6709\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.8131\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 3.0048\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 3.3827\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 2.2158\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.8340\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.9431\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 4.1020\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.9358\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.7859\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 3.4948\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.4313\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.2769\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.0751\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.0065\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 3.5169\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.8368\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.6995\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.6903\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.1843\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 2.2244\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.5345\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.5104\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8893\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.6090\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.2124\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.9241\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.2885\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.4742\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.1165\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.2625\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.2875\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.0699\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9574\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.2211\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.2750\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.5963\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.9526\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 2.6289\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.2754\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0604\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7145\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.2595\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.0021\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8811\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.8261\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.1764\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.6026\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.4585\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1867\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.4308\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6058\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5004\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.3477\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9573\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5908\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1748\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1115\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1625\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8852\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.2103\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8344\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.4812\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.5513\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.4766\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6064\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3561\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8150\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.3125\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6792\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.8059\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7542\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4824\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7892\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.8665\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.5776\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8254\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.1998\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1085\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8048\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.2857\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 3.3558\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7181\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8589\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1186\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.3158\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.3105\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6326\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6433\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.0156\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1296\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.8288\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7434\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1840\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1965\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5223\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9263\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0826\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5142\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7010\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8001\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.8322\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.6868\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.6755\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5743\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7549\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.8141\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.8388\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6585\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4950\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1006\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7724\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2984\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.7871\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6652\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5253\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.5088\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9702\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.4059\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5248\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2086\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5861\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6244\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5849\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.4137\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.2161\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.3315\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.4146\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3749\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3153\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4206\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5849\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3232\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3024\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5466\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6219\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.5212\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6814\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6871\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8037\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.4333\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.9082\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.3497\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7629\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.9262\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6514\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7347\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7446\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.4659\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6320\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.2246\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.2376\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.1846\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6056\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.2565\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.4815\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6573\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.5681\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.0017\n",
            "1/1 [==============================] - 0s 120ms/step\n",
            "Neural Network Mean Squared Error: 10.410445403850796\n",
            "Neural Network R-squared: 0.6477668848321159\n",
            "XGBoost Mean Squared Error: 10.047156566404908\n",
            "XGBoost R-squared: 0.6600586124148715\n",
            "Neural Network Predicted strikeout: 15.408795\n",
            "XGBoost Predicted strikeout: 9.021846\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ERICK FEDDE"
      ],
      "metadata": {
        "id": "AhrCtgkdbe5L"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Organize the data into a DataFrame\n",
        "data = {\n",
        "    \"innings_pitched\": [7, 6, 7, 5, 5, 6.1, 6, 7, 6, 4.1, 8.1, 6, 5.2, 5, 5, 4.2],\n",
        "    \"total_batter_faced\": [26, 26, 26, 22, 27, 25, 25, 24, 24, 22, 32, 21, 22, 24, 23, 20],\n",
        "    \"strikeouts_per_nine_inning\": [3.86, 9, 5.14, 12.6, 14.4, 8.53, 3, 7.71, 4.5, 4.15, 9.72, 16.5, 7.94, 5.4, 7.2, 13.5],\n",
        "    \"strikeouts\": [3, 6, 4, 7, 8, 6, 2, 6, 3, 2, 9, 11, 5, 3, 4, 7]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the independent variables and the dependent variable\n",
        "X = df[['innings_pitched', 'total_batter_faced', 'strikeouts_per_nine_inning']].values\n",
        "y = df['strikeouts'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for the neural network\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with a modified learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the neural network model\n",
        "model.fit(X_train_scaled, y_train, epochs=200, batch_size=4, verbose=1)\n",
        "\n",
        "# Predict on the test set with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate performance metrics for the neural network\n",
        "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
        "r2_nn = r2_score(y_test, y_pred_nn)\n",
        "\n",
        "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n",
        "print(f\"Neural Network R-squared: {r2_nn}\")\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.7\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgboost_model = xgb.train(params, dtrain, num_boost_round=200)\n",
        "\n",
        "# Predict on the test set with XGBoost\n",
        "y_pred_xgb = xgboost_model.predict(dtest)\n",
        "\n",
        "# Calculate performance metrics for XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb}\")\n",
        "print(f\"XGBoost R-squared: {r2_xgb}\")\n",
        "\n",
        "# Output the first model prediction\n",
        "print(\"Neural Network Predicted strikeout:\", y_pred_nn.flatten()[0])\n",
        "print(\"XGBoost Predicted strikeout:\", y_pred_xgb[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "a_KORaV_bhgO",
        "outputId": "86f582db-5fa8-4d8d-f163-3dd127f58437"
      },
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "3/3 [==============================] - 2s 8ms/step - loss: 39.5449\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 37.7743\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 36.0656\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 35.5457\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 34.1852\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 33.1771\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 30.8044\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 27.8119\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 26.1829\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 22.9702\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 11ms/step - loss: 19.5961\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 18.6167\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 15.5420\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 12.9021\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 10.9747\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 9.3804\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 7.3078\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 6.1690\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 6.6291\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 3.9758\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 15ms/step - loss: 3.1994\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 16ms/step - loss: 2.7181\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 18ms/step - loss: 2.5627\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 13ms/step - loss: 2.6622\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 5.1034\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 2.3209\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 1.6936\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 2.4102\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.0389\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.0204\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 2.0993\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 2.2404\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 2.6167\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.2186\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.9471\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.0114\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.5106\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.9231\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.6121\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.7821\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9124\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.4891\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.1546\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.5400\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.2265\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.1853\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.8076\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.5987\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.6943\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.3094\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.7215\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.2410\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.6964\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.3903\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.0970\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.5735\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6632\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1621\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.6650\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.3304\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9811\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.4607\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 1.1853\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0642\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5708\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9329\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.9369\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.4446\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.4337\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8060\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.9409\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4960\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0159\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0971\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.3429\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9138\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.8273\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.0219\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.2350\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.8497\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0873\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8294\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8998\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7931\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.2353\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.6128\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.1727\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.5201\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5513\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.4394\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6791\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7348\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.4341\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 2.8658\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9145\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2471\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8138\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.7415\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6547\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7458\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.2314\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1464\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.3713\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0339\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.5894\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.7268\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.7095\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.5666\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.6681\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.6607\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.7652\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.7810\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.2956\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.7517\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.7377\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4094\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.7531\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5229\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.0899\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.4922\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7011\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6585\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8246\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.9859\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.9492\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.8228\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.4007\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.4690\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5540\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7556\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6298\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3847\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.0287\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3019\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8765\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6953\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6149\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4606\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2789\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.7021\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6343\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6358\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.5060\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6172\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4302\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9039\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.4911\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.3106\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3860\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1679\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0125\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.3652\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6734\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3141\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5737\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6289\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1943\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7768\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6704\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8173\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.5766\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.7622\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0191\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3481\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6083\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7174\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9266\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4309\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0132\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2222\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.5249\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6922\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8417\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6148\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5117\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3299\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.7192\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3376\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.5422\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.5918\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.6228\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.2337\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.8505\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3402\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.7816\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.3355\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.8071\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.4824\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.9689\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.5394\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3207\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3708\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.4735\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3211\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.6042\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.8616\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8188\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.4256\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.1138\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.3813\n",
            "1/1 [==============================] - 0s 119ms/step\n",
            "Neural Network Mean Squared Error: 1.8100900529510113\n",
            "Neural Network R-squared: -0.07264595730430301\n",
            "XGBoost Mean Squared Error: 0.5213080699410426\n",
            "XGBoost R-squared: 0.691076699294197\n",
            "Neural Network Predicted strikeout: 3.2155833\n",
            "XGBoost Predicted strikeout: 3.2251902\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "SIMEON WOOD"
      ],
      "metadata": {
        "id": "8WKmYRiEcz01"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Organize the data into a DataFrame\n",
        "data = {\n",
        "    'innings_pitched': [6, 4, 6.1, 4.1, 5, 4.2, 5.1, 4.1, 6, 3.2, 5, 6],\n",
        "    'total_batter_faced': [24, 20, 26, 18, 20, 18, 21, 21, 20, 19, 23, 21],\n",
        "    'strikeouts_per_nine_inning': [9, 9, 8.53, 12.46, 7.2, 1.93, 3.38, 4.15, 12, 4.91, 10.8, 7.5],\n",
        "    'strikeouts': [6, 4, 6, 6, 4, 1, 2, 2, 8, 2, 6, 5]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the independent variables and the dependent variable\n",
        "X = df[['innings_pitched', 'total_batter_faced', 'strikeouts_per_nine_inning']].values\n",
        "y = df['strikeouts'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for the neural network\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with a modified learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the neural network model\n",
        "model.fit(X_train_scaled, y_train, epochs=200, batch_size=4, verbose=1)\n",
        "\n",
        "# Predict on the test set with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate performance metrics for the neural network\n",
        "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
        "r2_nn = r2_score(y_test, y_pred_nn)\n",
        "\n",
        "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n",
        "print(f\"Neural Network R-squared: {r2_nn}\")\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.7\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgboost_model = xgb.train(params, dtrain, num_boost_round=200)\n",
        "\n",
        "# Predict on the test set with XGBoost\n",
        "y_pred_xgb = xgboost_model.predict(dtest)\n",
        "\n",
        "# Calculate performance metrics for XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb}\")\n",
        "print(f\"XGBoost R-squared: {r2_xgb}\")\n",
        "\n",
        "# Output the first model prediction\n",
        "print(\"Neural Network Predicted strikeout:\", y_pred_nn.flatten()[0])\n",
        "print(\"XGBoost Predicted strikeout:\", y_pred_xgb[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YnD3WBlEc18B",
        "outputId": "e4888083-1ed9-4113-8a83-5527e793b2aa"
      },
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "3/3 [==============================] - 1s 6ms/step - loss: 21.7189\n",
            "Epoch 2/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 20.2661\n",
            "Epoch 3/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 19.3944\n",
            "Epoch 4/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 18.4907\n",
            "Epoch 5/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 17.7525\n",
            "Epoch 6/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 15.7592\n",
            "Epoch 7/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 14.8641\n",
            "Epoch 8/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 14.8952\n",
            "Epoch 9/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 13.1966\n",
            "Epoch 10/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 11.9180\n",
            "Epoch 11/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 9.9633\n",
            "Epoch 12/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 9.7459\n",
            "Epoch 13/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 9.0826\n",
            "Epoch 14/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 7.2394\n",
            "Epoch 15/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 5.2325\n",
            "Epoch 16/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 4.4580\n",
            "Epoch 17/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 5.2146\n",
            "Epoch 18/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 3.5947\n",
            "Epoch 19/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 3.2653\n",
            "Epoch 20/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 4.1912\n",
            "Epoch 21/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 2.2863\n",
            "Epoch 22/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 4.0319\n",
            "Epoch 23/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 2.3722\n",
            "Epoch 24/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 3.5743\n",
            "Epoch 25/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.4845\n",
            "Epoch 26/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.8042\n",
            "Epoch 27/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.9078\n",
            "Epoch 28/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.4513\n",
            "Epoch 29/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.6415\n",
            "Epoch 30/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7209\n",
            "Epoch 31/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0909\n",
            "Epoch 32/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0414\n",
            "Epoch 33/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.8218\n",
            "Epoch 34/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.0872\n",
            "Epoch 35/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.9630\n",
            "Epoch 36/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.1391\n",
            "Epoch 37/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.4199\n",
            "Epoch 38/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1074\n",
            "Epoch 39/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.4061\n",
            "Epoch 40/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.2844\n",
            "Epoch 41/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6243\n",
            "Epoch 42/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8282\n",
            "Epoch 43/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0500\n",
            "Epoch 44/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.9666\n",
            "Epoch 45/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.5670\n",
            "Epoch 46/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9985\n",
            "Epoch 47/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 1.3933\n",
            "Epoch 48/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4891\n",
            "Epoch 49/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7238\n",
            "Epoch 50/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8988\n",
            "Epoch 51/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6890\n",
            "Epoch 52/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6398\n",
            "Epoch 53/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 1.5736\n",
            "Epoch 54/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.3627\n",
            "Epoch 55/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.7379\n",
            "Epoch 56/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7625\n",
            "Epoch 57/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.6280\n",
            "Epoch 58/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8754\n",
            "Epoch 59/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8111\n",
            "Epoch 60/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7447\n",
            "Epoch 61/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7370\n",
            "Epoch 62/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0886\n",
            "Epoch 63/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8872\n",
            "Epoch 64/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6827\n",
            "Epoch 65/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6322\n",
            "Epoch 66/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0142\n",
            "Epoch 67/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.5403\n",
            "Epoch 68/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9813\n",
            "Epoch 69/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1322\n",
            "Epoch 70/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4778\n",
            "Epoch 71/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5007\n",
            "Epoch 72/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.1080\n",
            "Epoch 73/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.0269\n",
            "Epoch 74/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6344\n",
            "Epoch 75/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9687\n",
            "Epoch 76/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5531\n",
            "Epoch 77/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6767\n",
            "Epoch 78/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3891\n",
            "Epoch 79/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3969\n",
            "Epoch 80/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.5439\n",
            "Epoch 81/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5023\n",
            "Epoch 82/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5814\n",
            "Epoch 83/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.5406\n",
            "Epoch 84/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.7418\n",
            "Epoch 85/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7285\n",
            "Epoch 86/200\n",
            "3/3 [==============================] - 0s 10ms/step - loss: 0.3763\n",
            "Epoch 87/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4231\n",
            "Epoch 88/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6216\n",
            "Epoch 89/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8864\n",
            "Epoch 90/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.4667\n",
            "Epoch 91/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6373\n",
            "Epoch 92/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6349\n",
            "Epoch 93/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6117\n",
            "Epoch 94/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.3749\n",
            "Epoch 95/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6758\n",
            "Epoch 96/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 1.2628\n",
            "Epoch 97/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4535\n",
            "Epoch 98/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.2309\n",
            "Epoch 99/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6434\n",
            "Epoch 100/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.5501\n",
            "Epoch 101/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 1.2663\n",
            "Epoch 102/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5653\n",
            "Epoch 103/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4920\n",
            "Epoch 104/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8180\n",
            "Epoch 105/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8112\n",
            "Epoch 106/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.5605\n",
            "Epoch 107/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3671\n",
            "Epoch 108/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 1.0149\n",
            "Epoch 109/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.3805\n",
            "Epoch 110/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8072\n",
            "Epoch 111/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3164\n",
            "Epoch 112/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.3348\n",
            "Epoch 113/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3990\n",
            "Epoch 114/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5662\n",
            "Epoch 115/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4241\n",
            "Epoch 116/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5272\n",
            "Epoch 117/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.8671\n",
            "Epoch 118/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.6603\n",
            "Epoch 119/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.3914\n",
            "Epoch 120/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3079\n",
            "Epoch 121/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.1613\n",
            "Epoch 122/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.2824\n",
            "Epoch 123/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.4482\n",
            "Epoch 124/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6623\n",
            "Epoch 125/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4719\n",
            "Epoch 126/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.8315\n",
            "Epoch 127/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.5255\n",
            "Epoch 128/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9241\n",
            "Epoch 129/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7170\n",
            "Epoch 130/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3480\n",
            "Epoch 131/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3173\n",
            "Epoch 132/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.2883\n",
            "Epoch 133/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.5432\n",
            "Epoch 134/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.9052\n",
            "Epoch 135/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6752\n",
            "Epoch 136/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6049\n",
            "Epoch 137/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7023\n",
            "Epoch 138/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2316\n",
            "Epoch 139/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.5227\n",
            "Epoch 140/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.5500\n",
            "Epoch 141/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2860\n",
            "Epoch 142/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3010\n",
            "Epoch 143/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6348\n",
            "Epoch 144/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3353\n",
            "Epoch 145/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.3296\n",
            "Epoch 146/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.0963\n",
            "Epoch 147/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.1821\n",
            "Epoch 148/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.7699\n",
            "Epoch 149/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1660\n",
            "Epoch 150/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.5490\n",
            "Epoch 151/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.7368\n",
            "Epoch 152/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3596\n",
            "Epoch 153/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2366\n",
            "Epoch 154/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2403\n",
            "Epoch 155/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2671\n",
            "Epoch 156/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.4928\n",
            "Epoch 157/200\n",
            "3/3 [==============================] - 0s 9ms/step - loss: 0.3920\n",
            "Epoch 158/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.4958\n",
            "Epoch 159/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.4705\n",
            "Epoch 160/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1817\n",
            "Epoch 161/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.1524\n",
            "Epoch 162/200\n",
            "3/3 [==============================] - 0s 6ms/step - loss: 0.3752\n",
            "Epoch 163/200\n",
            "3/3 [==============================] - 0s 8ms/step - loss: 0.3124\n",
            "Epoch 164/200\n",
            "3/3 [==============================] - 0s 7ms/step - loss: 0.4820\n",
            "Epoch 165/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3910\n",
            "Epoch 166/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2968\n",
            "Epoch 167/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4040\n",
            "Epoch 168/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2747\n",
            "Epoch 169/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3929\n",
            "Epoch 170/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3126\n",
            "Epoch 171/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2153\n",
            "Epoch 172/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3055\n",
            "Epoch 173/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.8373\n",
            "Epoch 174/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3920\n",
            "Epoch 175/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2356\n",
            "Epoch 176/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2404\n",
            "Epoch 177/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1083\n",
            "Epoch 178/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2130\n",
            "Epoch 179/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3286\n",
            "Epoch 180/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.1395\n",
            "Epoch 181/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2195\n",
            "Epoch 182/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2838\n",
            "Epoch 183/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3470\n",
            "Epoch 184/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6017\n",
            "Epoch 185/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.5556\n",
            "Epoch 186/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.0781\n",
            "Epoch 187/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2069\n",
            "Epoch 188/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3251\n",
            "Epoch 189/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.3002\n",
            "Epoch 190/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.3846\n",
            "Epoch 191/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.4719\n",
            "Epoch 192/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.3509\n",
            "Epoch 193/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.5319\n",
            "Epoch 194/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.4501\n",
            "Epoch 195/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.4108\n",
            "Epoch 196/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.7369\n",
            "Epoch 197/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2102\n",
            "Epoch 198/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.6053\n",
            "Epoch 199/200\n",
            "3/3 [==============================] - 0s 5ms/step - loss: 0.2423\n",
            "Epoch 200/200\n",
            "3/3 [==============================] - 0s 4ms/step - loss: 0.1694\n",
            "1/1 [==============================] - 0s 70ms/step\n",
            "Neural Network Mean Squared Error: 0.45738017277489007\n",
            "Neural Network R-squared: 0.8713618264070622\n",
            "XGBoost Mean Squared Error: 0.2843945822459896\n",
            "XGBoost R-squared: 0.9200140237433154\n",
            "Neural Network Predicted strikeout: 5.3953733\n",
            "XGBoost Predicted strikeout: 5.1589055\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "R NELSON"
      ],
      "metadata": {
        "id": "sKLNiX3Qdsl3"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Organize the data into a DataFrame\n",
        "data = {\n",
        "    'innings_pitched': [7, 6, 3.1, 7.2, 6, 4, 4.2, 5, 6, 5, 2.2],\n",
        "    'total_batter_faced': [24, 24, 21, 27, 25, 25, 22, 23, 26, 21, 17],\n",
        "    'strikeouts_per_nine_inning': [2.57, 12, 2.7, 1.17, 4.5, 9, 5.79, 5.4, 6, 12.6, 6.75],\n",
        "    'strikeouts': [2, 8, 1, 1, 3, 4, 3, 3, 4, 7, 2]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the independent variables and the dependent variable\n",
        "X = df[['innings_pitched', 'total_batter_faced', 'strikeouts_per_nine_inning']].values\n",
        "y = df['strikeouts'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for the neural network\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with a modified learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the neural network model\n",
        "model.fit(X_train_scaled, y_train, epochs=200, batch_size=4, verbose=1)\n",
        "\n",
        "# Predict on the test set with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate performance metrics for the neural network\n",
        "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
        "r2_nn = r2_score(y_test, y_pred_nn)\n",
        "\n",
        "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n",
        "print(f\"Neural Network R-squared: {r2_nn}\")\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.7\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgboost_model = xgb.train(params, dtrain, num_boost_round=200)\n",
        "\n",
        "# Predict on the test set with XGBoost\n",
        "y_pred_xgb = xgboost_model.predict(dtest)\n",
        "\n",
        "# Calculate performance metrics for XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb}\")\n",
        "print(f\"XGBoost R-squared: {r2_xgb}\")\n",
        "\n",
        "# Output the first model prediction\n",
        "print(\"Neural Network Predicted strikeout:\", y_pred_nn.flatten()[0])\n",
        "print(\"XGBoost Predicted strikeout:\", y_pred_xgb[0])\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "xBhXif03dt6n",
        "outputId": "93fa3563-7b57-4971-c645-7ef203ca1e24"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 2s 8ms/step - loss: 11.9458\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 12.0669\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 11.2961\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 10.4325\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 9.3003\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 10.2798\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 9.2906\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 8.0245\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 7.0086\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 7.5219\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 6.9735\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 6.6496\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 6.6031\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 5.7939\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 5.8160\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 5.1128\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 4.1532\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 3.1964\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 4.9021\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 3.1474\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.9528\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 2.8324\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 3.3228\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 2.0416\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 2.3369\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.6177\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.6770\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 2.0379\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.2894\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.5819\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.7383\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.8832\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.3119\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.2364\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8990\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8571\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 2.3188\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7839\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.9322\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7193\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.1654\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.3217\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.5369\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.6753\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.9149\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0431\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.1098\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7725\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6001\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4771\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6155\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6268\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8448\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.9132\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6847\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5373\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3540\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6738\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.4721\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3536\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.8538\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4746\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5146\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5478\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.3485\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6995\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.8158\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5695\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5111\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4581\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3706\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8073\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1703\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8412\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2970\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3432\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4143\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2754\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2924\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2885\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3789\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2431\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3655\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.2813\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6739\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6807\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3897\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3096\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2187\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2285\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5494\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2556\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.1561\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.2291\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.0842\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2105\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4355\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.0759\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3513\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2562\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2503\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1678\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3870\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1740\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5137\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.7643\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5340\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1062\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1890\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0858\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1296\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.3145\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 1.0067\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3490\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1289\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4683\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1486\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2683\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2085\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1703\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1200\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1632\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.1946\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4883\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2506\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1647\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1003\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.0944\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.3013\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7228\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2129\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1919\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1466\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4232\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3406\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.3756\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0986\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4200\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1185\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3199\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4301\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2638\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2009\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3074\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.3484\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1133\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1795\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4107\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7907\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.0925\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.0508\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3142\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5105\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6981\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1534\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0949\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1059\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4783\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2882\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.0590\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1141\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1820\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1010\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2070\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1595\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1653\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.0626\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.1667\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1114\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2377\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2413\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1434\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.0997\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0767\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.0674\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1336\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.3243\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2963\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4310\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2770\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1166\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6993\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5145\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2961\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2702\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1735\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1884\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5746\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1734\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2778\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.0285\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1230\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1696\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.0958\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3086\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1986\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1809\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0338\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1751\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2238\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7bbab14edfc0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 93ms/step\n",
            "Neural Network Mean Squared Error: 0.6303419917609716\n",
            "Neural Network R-squared: 0.8507084756355594\n",
            "XGBoost Mean Squared Error: 1.2363567373414146\n",
            "XGBoost R-squared: 0.7071786674717702\n",
            "Neural Network Predicted strikeout: 5.2758727\n",
            "XGBoost Predicted strikeout: 2.3375583\n"
          ]
        }
      ]
    }
  ]
}