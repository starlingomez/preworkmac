{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyOlQIKTit0/AIDReUgyICcx",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/starlingomez/preworkmac/blob/master/june29th.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "ZAC GALLEN"
      ],
      "metadata": {
        "id": "bRldbFljV5Cp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "StUGN3hyV8zP"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f2cd70c-fc76-40d1-d337-46e4d0472e39",
        "id": "b-Km3HFV_qXK"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 3s 36ms/step - loss: 35.8841\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 34.3499\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 34.6150\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 33.0165\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 32.7517\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 31.5286\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 31.0681\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 30.9877\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 28.8793\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 28.7548\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 27.4908\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 26.4320\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 25.5584\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 24.5766\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 22.0463\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 19.8110\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 18.7405\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 17.7018\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 18.0022\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 17.2944\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 13.7078\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12.4136\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 9.7082\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 11.5085\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 9.2999\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 8.5576\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 7.7911\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 5.1305\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 4.1591\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4.5997\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 7.2712\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 4.1097\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 3.0901\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 3.5779\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4.4175\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 3.7981\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 3.3558\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2.3608\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2.5264\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 2.2780\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2.1591\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.6397\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 2.0300\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 4.0485\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.1299\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2.3695\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2.3764\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 2.3132\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 2.1225\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.9061\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.6925\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.4864\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.5045\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.3525\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.7561\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2.1691\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5831\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2.0371\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1.4981\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.7783\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1.6142\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1.8658\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.5453\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.5881\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3891\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.9089\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7387\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.0284\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.4909\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5196\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.6438\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.6183\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.0436\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.0460\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.9381\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.8988\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.3686\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7891\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1.2283\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5698\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.7504\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.0138\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5823\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.7647\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6669\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.2705\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.6232\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.6573\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.4968\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.8645\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1.5831\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5831\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.8598\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.9941\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.7012\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.6621\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1.0483\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1.7780\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.8250\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1.1180\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8449\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4730\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1.2217\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.4446\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.1102\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.6415\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2188\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.7661\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3511\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.9243\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.8496\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.6988\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.5317\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.1731\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4408\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.1544\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.7403\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.0518\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.4842\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.6348\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3359\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.5178\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4739\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.9997\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.8727\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 1.7863\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3956\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.8589\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.4726\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4603\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5274\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.6605\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3068\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5646\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1442\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.5059\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.7047\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.5218\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8851\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.3897\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.8754\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.4570\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.5410\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.7067\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0826\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3979\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2274\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.9648\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.6491\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.0066\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.5628\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.9749\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.2099\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.7670\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.9549\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.8456\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.8665\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2908\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.5793\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9182\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2058\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1969\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1521\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1027\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1.1026\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.1740\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.9520\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.9043\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.4902\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.8052\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4245\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5723\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.2906\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2654\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4719\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.6509\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.9876\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6636\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4547\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0744\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.4856\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.4781\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.5521\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1875\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3724\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.5621\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.3493\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1929\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 1.0750\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.2574\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.8128\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.3476\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.0973\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.9095\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.4895\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.6233\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1848\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.8787\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.6617\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1160\n",
            "1/1 [==============================] - 0s 477ms/step\n",
            "Neural Network Mean Squared Error: 3.2840762407346347\n",
            "Neural Network R-squared: 0.17898093981634133\n",
            "XGBoost Mean Squared Error: 6.396330455687462\n",
            "XGBoost R-squared: -0.5990826139218655\n",
            "Neural Network Predicted strikeout: 7.4433928\n",
            "XGBoost Predicted strikeout: 6.4233174\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Organize the data into a DataFrame\n",
        "data = {\n",
        "    \"innings_pitched\": [5, 6, 5, 6, 5, 5, 6, 6, 6.2, 7],\n",
        "    \"total_batter_faced\": [21, 22, 23, 23, 25, 19, 22, 23, 27, 27],\n",
        "    \"strikeouts_per_nine_inning\": [5.4, 9, 18, 10.5, 10.8, 5.4, 9, 6, 13.5, 6.4],\n",
        "    \"strikeouts\": [3, 6, 10, 7, 6, 3, 6, 4, 10, 5]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the independent variables and the dependent variable\n",
        "X = df[['innings_pitched', 'total_batter_faced', 'strikeouts_per_nine_inning']].values\n",
        "y = df['strikeouts'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for the neural network\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with a modified learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the neural network model\n",
        "model.fit(X_train_scaled, y_train, epochs=200, batch_size=4, verbose=1)\n",
        "\n",
        "# Predict on the test set with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate performance metrics for the neural network\n",
        "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
        "r2_nn = r2_score(y_test, y_pred_nn)\n",
        "\n",
        "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n",
        "print(f\"Neural Network R-squared: {r2_nn}\")\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.7\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgboost_model = xgb.train(params, dtrain, num_boost_round=200)\n",
        "\n",
        "# Predict on the test set with XGBoost\n",
        "y_pred_xgb = xgboost_model.predict(dtest)\n",
        "\n",
        "# Calculate performance metrics for XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb}\")\n",
        "print(f\"XGBoost R-squared: {r2_xgb}\")\n",
        "\n",
        "# Output the first model prediction\n",
        "print(\"Neural Network Predicted strikeout:\", y_pred_nn.flatten()[0])\n",
        "print(\"XGBoost Predicted strikeout:\", y_pred_xgb[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "A CIVALE"
      ],
      "metadata": {
        "id": "TQr36Ie9ZLUB"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "_JNQ_KzfZMw-"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "0f2cd70c-fc76-40d1-d337-46e4d0472e39",
        "id": "vIMGyV1KZPU7"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 3s 36ms/step - loss: 35.8841\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 34.3499\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 34.6150\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 33.0165\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 32.7517\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 31.5286\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 31.0681\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 41ms/step - loss: 30.9877\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 28.8793\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 61ms/step - loss: 28.7548\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 38ms/step - loss: 27.4908\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 26.4320\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 25.5584\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 24.5766\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 22.0463\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 19.8110\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 18.7405\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 17.7018\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 18.0022\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 17.2944\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 13.7078\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 12.4136\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 9.7082\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 11.5085\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 9.2999\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 8.5576\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 7.7911\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 5.1305\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 4.1591\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 4.5997\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 7.2712\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 28ms/step - loss: 4.1097\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 3.0901\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 3.5779\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 4.4175\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 49ms/step - loss: 3.7981\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 3.3558\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2.3608\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2.5264\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 2.2780\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2.1591\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 1.6397\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 2.0300\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 4.0485\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.1299\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2.3695\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 2.3764\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 2.3132\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 2.1225\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 37ms/step - loss: 0.9061\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.6925\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 1.4864\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.5045\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.3525\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.7561\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2.1691\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.5831\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2.0371\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1.4981\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.7783\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1.6142\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1.8658\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.5453\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.5881\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3891\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.9089\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7387\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.0284\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.4909\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5196\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.6438\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.6183\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.0436\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.0460\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.9381\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.8988\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.3686\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.7891\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1.2283\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.5698\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.7504\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 1.0138\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5823\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.7647\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.6669\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.2705\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.6232\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.6573\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.4968\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.8645\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 1.5831\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.5831\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.8598\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.9941\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.7012\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.6621\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1.0483\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 1.7780\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.8250\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 1.1180\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8449\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4730\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 1.2217\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 47ms/step - loss: 0.4446\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.1102\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.6415\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2188\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.7661\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3511\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.9243\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.8496\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.6988\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.5317\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.1731\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4408\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.1544\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 52ms/step - loss: 0.7403\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.0518\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 0.4842\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.6348\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3359\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 30ms/step - loss: 0.5178\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.4739\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.9997\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 51ms/step - loss: 0.8727\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 66ms/step - loss: 1.7863\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 36ms/step - loss: 0.3956\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.8589\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 40ms/step - loss: 0.4726\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4603\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 0.5274\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.6605\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3068\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5646\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1442\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.5059\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.7047\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.5218\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8851\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.3897\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.8754\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.4570\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.5410\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.7067\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0826\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3979\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 44ms/step - loss: 0.2274\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 45ms/step - loss: 0.9648\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 0.6491\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 1.0066\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 33ms/step - loss: 0.5628\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 26ms/step - loss: 0.9749\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 1.2099\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.7670\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.9549\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.8456\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.8665\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2908\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.5793\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9182\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.2058\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.1969\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1521\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1027\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 1.1026\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.1740\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.9520\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.9043\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.4902\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.8052\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.4245\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.5723\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 0.2906\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2654\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.4719\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.6509\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.9876\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 0.6636\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 24ms/step - loss: 0.4547\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.0744\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 35ms/step - loss: 0.4856\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.4781\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.5521\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1875\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.3724\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 43ms/step - loss: 0.5621\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 0.3493\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1929\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 31ms/step - loss: 1.0750\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.2574\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 60ms/step - loss: 0.8128\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 53ms/step - loss: 0.3476\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 34ms/step - loss: 1.0973\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.9095\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 27ms/step - loss: 0.4895\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.6233\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 32ms/step - loss: 0.1848\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.8787\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.6617\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1160\n",
            "1/1 [==============================] - 0s 477ms/step\n",
            "Neural Network Mean Squared Error: 3.2840762407346347\n",
            "Neural Network R-squared: 0.17898093981634133\n",
            "XGBoost Mean Squared Error: 6.396330455687462\n",
            "XGBoost R-squared: -0.5990826139218655\n",
            "Neural Network Predicted strikeout: 7.4433928\n",
            "XGBoost Predicted strikeout: 6.4233174\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Organize the data into a DataFrame\n",
        "data = {\n",
        "    \"innings_pitched\": [5, 6, 5, 6, 5, 5, 6, 6, 6.2, 7],\n",
        "    \"total_batter_faced\": [21, 22, 23, 23, 25, 19, 22, 23, 27, 27],\n",
        "    \"strikeouts_per_nine_inning\": [5.4, 9, 18, 10.5, 10.8, 5.4, 9, 6, 13.5, 6.4],\n",
        "    \"strikeouts\": [3, 6, 10, 7, 6, 3, 6, 4, 10, 5]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the independent variables and the dependent variable\n",
        "X = df[['innings_pitched', 'total_batter_faced', 'strikeouts_per_nine_inning']].values\n",
        "y = df['strikeouts'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for the neural network\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with a modified learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the neural network model\n",
        "model.fit(X_train_scaled, y_train, epochs=200, batch_size=4, verbose=1)\n",
        "\n",
        "# Predict on the test set with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate performance metrics for the neural network\n",
        "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
        "r2_nn = r2_score(y_test, y_pred_nn)\n",
        "\n",
        "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n",
        "print(f\"Neural Network R-squared: {r2_nn}\")\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.7\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgboost_model = xgb.train(params, dtrain, num_boost_round=200)\n",
        "\n",
        "# Predict on the test set with XGBoost\n",
        "y_pred_xgb = xgboost_model.predict(dtest)\n",
        "\n",
        "# Calculate performance metrics for XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb}\")\n",
        "print(f\"XGBoost R-squared: {r2_xgb}\")\n",
        "\n",
        "# Output the first model prediction\n",
        "print(\"Neural Network Predicted strikeout:\", y_pred_nn.flatten()[0])\n",
        "print(\"XGBoost Predicted strikeout:\", y_pred_xgb[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "CIVALE"
      ],
      "metadata": {
        "id": "rd0xdwdGZlip"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "ZfovvKQzZng9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4dac2703-0c6a-4318-881a-2dddf3739535",
        "id": "TyLxikaNZrUn"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 8ms/step - loss: 25.7716\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 24.8694\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 24.1285\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 23.2288\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 22.7938\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 21.3593\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 21.1405\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 19.9674\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 19.5784\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 17.6228\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 16.8455\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 15.5585\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 14.5507\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 13.2159\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 10.8086\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 11.3907\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 9.8109\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 9.0156\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 7.2593\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6.4258\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6.3340\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 4.8112\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.2908\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 4.4824\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5473\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.3129\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.6214\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3.1357\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.6804\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.0648\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.7308\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.5957\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0765\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1860\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1119\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0870\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1415\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.9229\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.0740\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.7335\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.7372\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0490\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0068\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.9195\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.1500\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.7761\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.7992\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.7014\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.6407\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.4395\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.6126\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.5812\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.5146\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.5578\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6436\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4495\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5142\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.1632\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.4955\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7885\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.7267\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8026\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.9511\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4691\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9785\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.6597\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.4691\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.6894\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.7193\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.6536\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.8396\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.8365\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.3988\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.6260\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.2091\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.6910\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.6309\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.1964\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.5670\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.5061\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.6561\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.4337\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6858\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3497\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4226\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0662\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5066\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6631\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8053\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3425\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.7664\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5755\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9202\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5462\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4036\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7442\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0365\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2201\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7360\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3669\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4022\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3570\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7455\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7422\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4197\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4148\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3410\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3158\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3044\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3475\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1790\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4758\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3546\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7958\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4185\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2308\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0325\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.0808\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.1353\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6501\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3778\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3905\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2780\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6687\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5611\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.9895\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5680\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6010\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6743\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2278\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1091\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2934\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1978\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4690\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7198\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.7024\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4788\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3355\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7342\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1651\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4071\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4609\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2019\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5042\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4657\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3607\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.8497\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.1009\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.4123\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.6283\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6825\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1761\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2862\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1525\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.4664\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5611\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2924\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.0767\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5340\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.8106\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2665\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4826\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3226\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3652\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7755\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4996\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2108\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3636\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6699\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3867\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.5389\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8995\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3778\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.6574\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3370\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2268\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3780\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.5882\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.0874\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.2206\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.0962\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.4067\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1125\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2004\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1269\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4701\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.5998\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 0.1888\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3713\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.1556\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 0.4842\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3113\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2224\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.5426\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3415\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5096\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.5274\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.5289\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 0.2945\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.7915\n",
            "1/1 [==============================] - 0s 153ms/step\n",
            "Neural Network Mean Squared Error: 10.582715758246195\n",
            "Neural Network R-squared: -41.33086303298478\n",
            "XGBoost Mean Squared Error: 0.6413773125067621\n",
            "XGBoost R-squared: -1.5655092500270484\n",
            "Neural Network Predicted strikeout: 4.217148\n",
            "XGBoost Predicted strikeout: 3.0917294\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Organize the data into a DataFrame\n",
        "data = {\n",
        "    'innings_pitched': [5, 3.2, 5.2, 5, 5.1, 5, 5.1, 5, 4.2, 4.2],\n",
        "    'total_batter_faced': [18, 20, 24, 24, 23, 20, 25, 20, 20, 21],\n",
        "    'strikeouts_per_nine_inning': [5.4, 12.3, 9.5, 14.4, 5.1, 9, 5.1, 10.8, 7.7, 9.6],\n",
        "    'strikeouts': [3, 5, 6, 8, 3, 5, 3, 6, 4, 5]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the independent variables and the dependent variable\n",
        "X = df[['innings_pitched', 'total_batter_faced', 'strikeouts_per_nine_inning']].values\n",
        "y = df['strikeouts'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for the neural network\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with a modified learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the neural network model\n",
        "model.fit(X_train_scaled, y_train, epochs=200, batch_size=4, verbose=1)\n",
        "\n",
        "# Predict on the test set with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate performance metrics for the neural network\n",
        "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
        "r2_nn = r2_score(y_test, y_pred_nn)\n",
        "\n",
        "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n",
        "print(f\"Neural Network R-squared: {r2_nn}\")\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.7\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgboost_model = xgb.train(params, dtrain, num_boost_round=200)\n",
        "\n",
        "# Predict on the test set with XGBoost\n",
        "y_pred_xgb = xgboost_model.predict(dtest)\n",
        "\n",
        "# Calculate performance metrics for XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb}\")\n",
        "print(f\"XGBoost R-squared: {r2_xgb}\")\n",
        "\n",
        "# Output the first model prediction\n",
        "print(\"Neural Network Predicted strikeout:\", y_pred_nn.flatten()[0])\n",
        "print(\"XGBoost Predicted strikeout:\", y_pred_xgb[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "VALDEZ"
      ],
      "metadata": {
        "id": "Y6g_mqkLazMI"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gujhj1UTa2qR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7f471fbf-9ddf-4f25-99ee-96080e3984d4",
        "id": "f4mqdtt2a29A"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 8ms/step - loss: 27.4380\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 26.3488\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 24.4980\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 23.6571\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 23.1497\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 22.9576\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 21.0830\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 20.2040\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 19.5655\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 17.8545\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 17.2346\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 15.6038\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 13.9781\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 15.2695\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 12.1910\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 9.9482\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 9.9547\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 8.4211\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 7.8950\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 6.5869\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 7.3254\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 5.7235\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 6.1250\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 4.4439\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.9839\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.9171\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.8802\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 3.7450\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.5124\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.3343\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.2251\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.4925\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.3727\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.5065\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.4537\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 4.5452\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.5917\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.6911\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.8775\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5002\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.8570\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.6189\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.1851\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.1667\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.7237\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2290\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.5470\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.4167\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0440\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.1008\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0007\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.6318\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.0178\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9049\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5734\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.0850\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5412\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5402\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9283\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9375\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9018\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8644\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0269\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0006\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9958\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8742\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3975\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.7991\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.3032\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6954\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6234\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5604\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.8056\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.3627\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5457\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8416\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6779\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8375\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.3853\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3022\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.0173\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8521\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4284\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5583\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2113\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 0.3522\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5219\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5322\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3396\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1496\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2719\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3831\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.3558\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4258\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1978\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6128\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.1672\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2526\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4243\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4121\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6410\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7586\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4322\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1697\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1431\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3506\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.9780\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.0916\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5692\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9396\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5189\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.1349\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4638\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2650\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5235\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6279\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3795\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8169\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3535\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2683\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0201\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5101\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.6594\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3910\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5337\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1706\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.3492\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3713\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3940\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1092\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.5137\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7706\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2198\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4189\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.2385\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.2186\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.0934\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1513\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4507\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5445\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4299\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3571\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.1381\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.8394\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5771\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.4051\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8170\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3136\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5551\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2658\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6052\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5338\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3733\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3159\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.1891\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4846\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.4383\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3442\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.1542\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2505\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4608\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8697\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.0834\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9093\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.7717\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4712\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.3190\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6350\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.3033\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3613\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5585\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3683\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2042\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.2087\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.3232\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2489\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2174\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.1142\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4484\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3720\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5913\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7395\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9563\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5434\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4926\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3070\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3869\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5170\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3836\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2411\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1377\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2730\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4329\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6314\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3332\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.7537\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3018\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6920\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7028\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5839\n",
            "1/1 [==============================] - 0s 76ms/step\n",
            "Neural Network Mean Squared Error: 0.7041622317857446\n",
            "Neural Network R-squared: -1.8166489271429782\n",
            "XGBoost Mean Squared Error: 4.238610862225414\n",
            "XGBoost R-squared: -15.954443448901657\n",
            "Neural Network Predicted strikeout: 4.6088576\n",
            "XGBoost Predicted strikeout: 5.633299\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Organize the data into a DataFrame\n",
        "data = {\n",
        "    \"innings_pitched\": [7, 6, 4, 9, 7, 6, 5, 7, 7, 5.1],\n",
        "    \"total_batter_faced\": [29, 24, 21, 31, 24, 27, 25, 24, 29, 25],\n",
        "    \"strikeouts_per_nine_inning\": [9, 7.5, 2.3, 8, 5.1, 6, 5.4, 10.3, 5.1, 5.1],\n",
        "    \"strikeouts\": [7, 5, 1, 8, 4, 4, 3, 8, 4, 3]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the independent variables and the dependent variable\n",
        "X = df[['innings_pitched', 'total_batter_faced', 'strikeouts_per_nine_inning']].values\n",
        "y = df['strikeouts'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for the neural network\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with a modified learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the neural network model\n",
        "model.fit(X_train_scaled, y_train, epochs=200, batch_size=4, verbose=1)\n",
        "\n",
        "# Predict on the test set with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate performance metrics for the neural network\n",
        "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
        "r2_nn = r2_score(y_test, y_pred_nn)\n",
        "\n",
        "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n",
        "print(f\"Neural Network R-squared: {r2_nn}\")\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.7\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgboost_model = xgb.train(params, dtrain, num_boost_round=200)\n",
        "\n",
        "# Predict on the test set with XGBoost\n",
        "y_pred_xgb = xgboost_model.predict(dtest)\n",
        "\n",
        "# Calculate performance metrics for XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb}\")\n",
        "print(f\"XGBoost R-squared: {r2_xgb}\")\n",
        "\n",
        "# Output the first model prediction\n",
        "print(\"Neural Network Predicted strikeout:\", y_pred_nn.flatten()[0])\n",
        "print(\"XGBoost Predicted strikeout:\", y_pred_xgb[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TANNER BIBEE"
      ],
      "metadata": {
        "id": "HS1rGkdFcIIq"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "s_400IcLcOor"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "07193616-0e85-4ce9-a131-0bf75328f84c",
        "id": "9zGtk477cO5k"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 10ms/step - loss: 50.8233\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 49.1480\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 46.2495\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 44.5212\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 44.2379\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 44.5628\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 42.4916\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 40.7606\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 39.2608\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 36.1680\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 35.0650\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 34.9288\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 34.7701\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 29.2794\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 26.7290\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 24.9597\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 25.3530\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 23.1114\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 20.7262\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 15.7288\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 15.8838\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 15.2406\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 8.8434\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 6.2565\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 6.5819\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 5.1064\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 5.0263\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 5.0698\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.0573\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6.9728\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.2129\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.1484\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 7.5010\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.2493\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 7.2329\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 4.9345\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.6775\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.1921\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.6897\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 3.6728\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 4.2041\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5852\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.4698\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.2921\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.6202\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.4478\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 5.1957\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4923\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.5977\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.4282\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.5156\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.0762\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.4159\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.1044\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.7673\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 2.7059\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 4.0898\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.2606\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.3177\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.1641\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.3121\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.8453\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 2.3649\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8053\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.5360\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.0125\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.5822\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5112\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.9508\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.1463\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1586\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.9330\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.1034\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.7240\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.6712\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.1787\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.6977\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.0131\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.6220\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.6292\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.6998\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.6611\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.9508\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.7478\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.5399\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.5155\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.8521\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.5538\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9594\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.0342\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.5869\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.7836\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8458\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7419\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.0555\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.4052\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.0516\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.5902\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.2573\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.8506\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.1802\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.5074\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.1421\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1964\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.8390\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.1467\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.1905\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.6327\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.5459\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8090\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.0952\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9560\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 1.1381\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.9254\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.2570\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.2248\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.0972\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4745\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.5425\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.8471\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.4121\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.9618\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.6410\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2756\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6209\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8129\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0818\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0647\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8096\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.6407\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4848\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3796\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.8725\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9764\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1954\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2978\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.4327\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.5364\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.8384\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9058\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.4152\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6858\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1806\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.1363\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9880\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.4952\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6767\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5150\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7002\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5851\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.4720\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8057\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5554\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.2903\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9899\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5972\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.2092\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.2984\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.6142\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0064\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.0610\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3324\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0773\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.2582\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5923\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9960\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7441\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.6949\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6554\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.4043\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9040\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5049\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1173\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9482\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1864\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.3591\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.3963\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.7446\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2891\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0029\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5514\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1052\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.2969\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.4031\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4514\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3649\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7551\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6361\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7795\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.0313\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3473\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.1316\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.5333\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.7277\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6890\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5458\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 0.9599\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.4270\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 0.4005\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 0.5351\n",
            "1/1 [==============================] - 0s 218ms/step\n",
            "Neural Network Mean Squared Error: 6.890373142473436\n",
            "Neural Network R-squared: 0.4375205597980869\n",
            "XGBoost Mean Squared Error: 11.004203013549159\n",
            "XGBoost R-squared: 0.10169771317966048\n",
            "Neural Network Predicted strikeout: 7.686015\n",
            "XGBoost Predicted strikeout: 6.950156\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Organize the data into a DataFrame\n",
        "data = {\n",
        "    'innings_pitched': [6, 6, 5.1, 5, 6.1, 6, 7, 5.1, 4, 5],\n",
        "    'total_batter_faced': [25, 22, 21, 24, 24, 25, 26, 21, 18, 23],\n",
        "    'strikeouts_per_nine_inning': [10.5, 18, 18.6, 12.6, 9.9, 9, 10.3, 6.8, 11.3, 7.2],\n",
        "    'strikeouts': [7, 12, 11, 7, 7, 6, 8, 4, 5, 4]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the independent variables and the dependent variable\n",
        "X = df[['innings_pitched', 'total_batter_faced', 'strikeouts_per_nine_inning']].values\n",
        "y = df['strikeouts'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for the neural network\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with a modified learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the neural network model\n",
        "model.fit(X_train_scaled, y_train, epochs=200, batch_size=4, verbose=1)\n",
        "\n",
        "# Predict on the test set with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate performance metrics for the neural network\n",
        "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
        "r2_nn = r2_score(y_test, y_pred_nn)\n",
        "\n",
        "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n",
        "print(f\"Neural Network R-squared: {r2_nn}\")\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.7\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgboost_model = xgb.train(params, dtrain, num_boost_round=200)\n",
        "\n",
        "# Predict on the test set with XGBoost\n",
        "y_pred_xgb = xgboost_model.predict(dtest)\n",
        "\n",
        "# Calculate performance metrics for XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb}\")\n",
        "print(f\"XGBoost R-squared: {r2_xgb}\")\n",
        "\n",
        "# Output the first model prediction\n",
        "print(\"Neural Network Predicted strikeout:\", y_pred_nn.flatten()[0])\n",
        "print(\"XGBoost Predicted strikeout:\", y_pred_xgb[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "COLE RAGAN"
      ],
      "metadata": {
        "id": "Y9xa8lJSceLP"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "f3aFT_lvcmUm"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "f4cff8a7-ec76-40c0-d191-3c0d2d196fff",
        "id": "iIwV5nTfcmlJ"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 3s 15ms/step - loss: 69.4278\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 68.5748\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 66.6192\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 66.2252\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 65.2865\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 64.2026\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 61.4500\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 60.3761\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 59.9141\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 59.4229\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 55.9204\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 53.3887\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 51.8591\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 50.3745\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 46.5309\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 45.6454\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 42.6902\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 39.3849\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 37.5524\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 31.9257\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 29.9452\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 25.0814\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 23.1092\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 20.4243\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 17.2387\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 16.0151\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 15.6232\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 10.0867\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 12.5557\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 8.0332\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8.3840\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 13.4134\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 4.4759\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 8.1379\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 6.1256\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 8.1124\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 6.5770\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 4.7052\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 8.2154\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 5.8808\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 7.6713\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 18ms/step - loss: 2.7760\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2.3917\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 22ms/step - loss: 5.7628\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 2.8215\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 5.8927\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.8738\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 4.5434\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.5549\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.0923\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 4.7401\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.4012\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 4.2843\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.9744\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.7780\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 4.4489\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 4.1465\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.5612\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.5790\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.5094\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.9052\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 1.6176\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 4.5535\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.0281\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.7977\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 19ms/step - loss: 3.6263\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 23ms/step - loss: 3.2497\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 25ms/step - loss: 2.9404\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 4.4623\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 5.5007\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 1.9728\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 3.0839\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.2359\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.0059\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 4.1550\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 3.4247\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.7931\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2.7801\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.2609\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 21ms/step - loss: 2.1759\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9972\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2.6934\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.8545\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 29ms/step - loss: 1.4518\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 39ms/step - loss: 2.4083\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 58ms/step - loss: 1.3059\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 57ms/step - loss: 0.9107\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 2.1046\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2.4486\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 17ms/step - loss: 2.0042\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.3283\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.4334\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.2337\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.4917\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3305\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.8316\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.4102\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.5044\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.0625\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.8370\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.0165\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.7614\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.2039\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.4660\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.2487\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.4854\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5675\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3341\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.3758\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6516\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.4690\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.0005\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5772\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.8104\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0378\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.9783\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8052\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.7569\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1580\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.8917\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.2802\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.7275\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5715\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.9007\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 4.1816\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.9652\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.0704\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.7012\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.4828\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.2772\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.9332\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9126\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9294\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.7082\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.8239\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.1340\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 3.1066\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7924\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4445\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3588\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.7086\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.1950\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.8384\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.3278\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 15ms/step - loss: 1.0296\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0335\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.1872\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.7456\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3399\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5314\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.6474\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6105\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4056\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.2391\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9418\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.7968\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7221\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6818\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5337\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0609\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9785\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.6060\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.9080\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.5211\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9817\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.9815\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1189\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.0456\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.8134\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.9979\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6411\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.8415\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.6422\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.8364\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9823\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 3.6669\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.2946\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.1433\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5617\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0248\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 5.5349\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.7762\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.0583\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.6686\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.2352\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5268\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6538\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.2959\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.3280\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 2.2493\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.4892\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1237\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.4428\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.2829\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5523\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.2258\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.3445\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.9202\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.9639\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.2917\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:5 out of the last 5 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7b135abf9870> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 94ms/step\n",
            "Neural Network Mean Squared Error: 11.56521238734831\n",
            "Neural Network R-squared: -1.8913030968370776\n",
            "XGBoost Mean Squared Error: 12.577423167580719\n",
            "XGBoost R-squared: -2.1443557918951797\n",
            "Neural Network Predicted strikeout: 6.454632\n",
            "XGBoost Predicted strikeout: 7.928724\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Organize the data into a DataFrame\n",
        "data = {\n",
        "    \"innings_pitched\": [6, 6.1, 7, 6, 5, 6, 6, 7, 6, 6],\n",
        "    \"total_batter_faced\": [22, 28, 23, 23, 24, 24, 22, 26, 26, 25],\n",
        "    \"strikeouts_per_nine_inning\": [12, 4.3, 9, 18, 12.6, 9, 13.5, 5.1, 10.5, 16.5],\n",
        "    \"strikeouts\": [11, 7, 4, 9, 6, 7, 12, 7, 3, 8]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the independent variables and the dependent variable\n",
        "X = df[['innings_pitched', 'total_batter_faced', 'strikeouts_per_nine_inning']].values\n",
        "y = df['strikeouts'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for the neural network\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with a modified learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the neural network model\n",
        "model.fit(X_train_scaled, y_train, epochs=200, batch_size=4, verbose=1)\n",
        "\n",
        "# Predict on the test set with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate performance metrics for the neural network\n",
        "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
        "r2_nn = r2_score(y_test, y_pred_nn)\n",
        "\n",
        "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n",
        "print(f\"Neural Network R-squared: {r2_nn}\")\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.7\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgboost_model = xgb.train(params, dtrain, num_boost_round=200)\n",
        "\n",
        "# Predict on the test set with XGBoost\n",
        "y_pred_xgb = xgboost_model.predict(dtest)\n",
        "\n",
        "# Calculate performance metrics for XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb}\")\n",
        "print(f\"XGBoost R-squared: {r2_xgb}\")\n",
        "\n",
        "# Output the first model prediction\n",
        "print(\"Neural Network Predicted strikeout:\", y_pred_nn.flatten()[0])\n",
        "print(\"XGBoost Predicted strikeout:\", y_pred_xgb[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "PAUL SKENES"
      ],
      "metadata": {
        "id": "3bsG848NeWgy"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "U2NUdzE3eYPr"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5c69bef4-cfda-411f-faeb-6b81c2b50671",
        "id": "rpekZMAxebS6"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 8ms/step - loss: 50.0294\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 51.1392\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 48.5671\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 48.8599\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 46.5642\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 45.9764\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 43.9521\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 43.4576\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 42.1963\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 40.1486\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 40.4415\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 39.6595\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 35.8620\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 33.3656\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 33.8332\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 28.3104\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 26.7588\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 27.3300\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 24.6741\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 24.3378\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 19.0377\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 16.7184\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 17.8137\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 17.1692\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 12.2416\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 11.3429\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 12.4485\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 10.8612\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 9.2427\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 9.8411\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 7.6744\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 13.1834\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 13.4788\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 11.5647\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 8.4151\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 5.3838\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 11.1466\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 6.7425\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 5.4447\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 4.4042\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 6.9109\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 9.4267\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 7.2494\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 5.4914\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 5.2837\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 4.1654\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 4.5461\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 6ms/step - loss: 6.4826\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 5.3598\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.4065\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 7.6017\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6.4091\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 6.4951\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 4.2382\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 5.7920\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 4.0428\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.1685\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.5985\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.8790\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.1276\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.9210\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 2.5480\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.0041\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.8077\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.2592\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.8012\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4574\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.6349\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1653\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6029\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.5979\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.4298\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9973\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.2763\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.2288\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.7101\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.2576\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.0557\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.0001\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5726\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.5770\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.3289\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4568\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.0611\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.3527\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.3607\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5295\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.6508\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.8199\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.1722\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.8049\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5509\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9254\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.3574\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.2542\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1498\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.7974\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7431\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.2152\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0535\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9932\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.6088\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 4.0747\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.6146\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1300\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.8934\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.4123\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.3649\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.0498\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.3254\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.9527\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.3219\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.8978\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.0969\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.7348\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.4550\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7484\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.9991\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0143\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.3341\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.2597\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.5528\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4551\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.7593\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1156\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9605\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4859\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6749\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7065\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6936\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.0088\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.2535\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0476\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9471\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.1215\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5537\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4518\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6023\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.4144\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3717\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5186\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3273\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.8241\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6551\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6530\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7381\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.1717\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5169\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.1000\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.2006\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.5521\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.1146\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.1317\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4175\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.6794\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.7104\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9079\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.4595\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.3589\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6527\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1606\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4317\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1043\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.6209\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5094\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.2323\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.2611\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.0302\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0971\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.7051\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.6407\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.1560\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.4329\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5576\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2961\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.8076\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 0.8899\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.4568\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.3965\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.6789\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9543\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9839\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.9455\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 14ms/step - loss: 0.9547\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 16ms/step - loss: 0.8187\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8880\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.8106\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9510\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.4813\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.6428\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.8829\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.5739\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.4567\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.0466\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.9887\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.3631\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.0441\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1752\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3327\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1130\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "WARNING:tensorflow:6 out of the last 6 calls to <function Model.make_predict_function.<locals>.predict_function at 0x7b135abfb6d0> triggered tf.function retracing. Tracing is expensive and the excessive number of tracings could be due to (1) creating @tf.function repeatedly in a loop, (2) passing tensors with different shapes, (3) passing Python objects instead of tensors. For (1), please define your @tf.function outside of the loop. For (2), @tf.function has reduce_retracing=True option that can avoid unnecessary retracing. For (3), please refer to https://www.tensorflow.org/guide/function#controlling_retracing and https://www.tensorflow.org/api_docs/python/tf/function for  more details.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "1/1 [==============================] - 0s 108ms/step\n",
            "Neural Network Mean Squared Error: 3.4574039143667505\n",
            "Neural Network R-squared: -0.536623961940778\n",
            "XGBoost Mean Squared Error: 2.0084354732449583\n",
            "XGBoost R-squared: 0.1073620118911296\n",
            "Neural Network Predicted strikeout: 13.319859\n",
            "XGBoost Predicted strikeout: 8.997043\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Organize the data into a DataFrame\n",
        "data = {\n",
        "    \"innings_pitched\": [4, 6, 6, 6, 5, 6.1, 6, 7],\n",
        "    \"total_batter_faced\": [20, 19, 23, 22, 23, 23, 24, 27],\n",
        "    \"strikeouts_per_nine_inning\": [15.8, 16.5, 4.5, 13.5, 14.4, 11.4, 10.5, 10.3],\n",
        "    \"strikeouts\": [7, 11, 3, 9, 8, 8, 7, 8]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the independent variables and the dependent variable\n",
        "X = df[['innings_pitched', 'total_batter_faced', 'strikeouts_per_nine_inning']].values\n",
        "y = df['strikeouts'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for the neural network\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with a modified learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the neural network model\n",
        "model.fit(X_train_scaled, y_train, epochs=200, batch_size=4, verbose=1)\n",
        "\n",
        "# Predict on the test set with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate performance metrics for the neural network\n",
        "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
        "r2_nn = r2_score(y_test, y_pred_nn)\n",
        "\n",
        "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n",
        "print(f\"Neural Network R-squared: {r2_nn}\")\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.7\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgboost_model = xgb.train(params, dtrain, num_boost_round=200)\n",
        "\n",
        "# Predict on the test set with XGBoost\n",
        "y_pred_xgb = xgboost_model.predict(dtest)\n",
        "\n",
        "# Calculate performance metrics for XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb}\")\n",
        "print(f\"XGBoost R-squared: {r2_xgb}\")\n",
        "\n",
        "# Output the first model prediction\n",
        "print(\"Neural Network Predicted strikeout:\", y_pred_nn.flatten()[0])\n",
        "print(\"XGBoost Predicted strikeout:\", y_pred_xgb[0])\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "TYLER GLASNOW"
      ],
      "metadata": {
        "id": "EOh6NFGXgpg_"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "VT7ZKah3grRv"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "61e15590-5c22-4ea6-c07f-71404ba5670d",
        "id": "_MFEKh5Pgrhe"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 1/200\n",
            "2/2 [==============================] - 1s 13ms/step - loss: 84.6004\n",
            "Epoch 2/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 82.5631\n",
            "Epoch 3/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 79.4357\n",
            "Epoch 4/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 80.4478\n",
            "Epoch 5/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 78.4772\n",
            "Epoch 6/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 76.9667\n",
            "Epoch 7/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 73.6993\n",
            "Epoch 8/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 71.4727\n",
            "Epoch 9/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 70.2128\n",
            "Epoch 10/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 70.5447\n",
            "Epoch 11/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 67.0862\n",
            "Epoch 12/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 65.7881\n",
            "Epoch 13/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 60.1503\n",
            "Epoch 14/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 59.5292\n",
            "Epoch 15/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 55.0027\n",
            "Epoch 16/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 54.4519\n",
            "Epoch 17/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 49.6931\n",
            "Epoch 18/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 49.2801\n",
            "Epoch 19/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 43.8701\n",
            "Epoch 20/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 38.1857\n",
            "Epoch 21/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 39.4826\n",
            "Epoch 22/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 35.8024\n",
            "Epoch 23/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 28.1697\n",
            "Epoch 24/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 27.4340\n",
            "Epoch 25/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 20.4615\n",
            "Epoch 26/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 20.6035\n",
            "Epoch 27/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 16.0530\n",
            "Epoch 28/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 13.4691\n",
            "Epoch 29/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 14.4987\n",
            "Epoch 30/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 12.9200\n",
            "Epoch 31/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 11.9054\n",
            "Epoch 32/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 10.4637\n",
            "Epoch 33/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 7.8674\n",
            "Epoch 34/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 13.9712\n",
            "Epoch 35/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 15.4387\n",
            "Epoch 36/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 16.0790\n",
            "Epoch 37/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 11.8163\n",
            "Epoch 38/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 12.3424\n",
            "Epoch 39/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 14.2795\n",
            "Epoch 40/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 12.2504\n",
            "Epoch 41/200\n",
            "2/2 [==============================] - 0s 20ms/step - loss: 13.1340\n",
            "Epoch 42/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 8.2775\n",
            "Epoch 43/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 8.1041\n",
            "Epoch 44/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 7.8297\n",
            "Epoch 45/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 9.7040\n",
            "Epoch 46/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 6.4393\n",
            "Epoch 47/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 5.4651\n",
            "Epoch 48/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 7.9262\n",
            "Epoch 49/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 5.8074\n",
            "Epoch 50/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 8.6248\n",
            "Epoch 51/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6.9573\n",
            "Epoch 52/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 6.6097\n",
            "Epoch 53/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 5.2699\n",
            "Epoch 54/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 7.6406\n",
            "Epoch 55/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 5.3957\n",
            "Epoch 56/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 4.3288\n",
            "Epoch 57/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.2545\n",
            "Epoch 58/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 4.3977\n",
            "Epoch 59/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 8.3727\n",
            "Epoch 60/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 4.4019\n",
            "Epoch 61/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 5.7989\n",
            "Epoch 62/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 4.1139\n",
            "Epoch 63/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 4.1348\n",
            "Epoch 64/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 3.9411\n",
            "Epoch 65/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 7.2917\n",
            "Epoch 66/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 4.9863\n",
            "Epoch 67/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.5522\n",
            "Epoch 68/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 7.1620\n",
            "Epoch 69/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.7826\n",
            "Epoch 70/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 4.7959\n",
            "Epoch 71/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 5.2994\n",
            "Epoch 72/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 4.5117\n",
            "Epoch 73/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 4.4388\n",
            "Epoch 74/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.1635\n",
            "Epoch 75/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.8773\n",
            "Epoch 76/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 3.0754\n",
            "Epoch 77/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 2.1365\n",
            "Epoch 78/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 3.0210\n",
            "Epoch 79/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 4.1266\n",
            "Epoch 80/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 4.4072\n",
            "Epoch 81/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.5353\n",
            "Epoch 82/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 4.6786\n",
            "Epoch 83/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.4914\n",
            "Epoch 84/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.0010\n",
            "Epoch 85/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.6813\n",
            "Epoch 86/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.9194\n",
            "Epoch 87/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.1792\n",
            "Epoch 88/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.7754\n",
            "Epoch 89/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 4.3051\n",
            "Epoch 90/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.2226\n",
            "Epoch 91/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 4.4348\n",
            "Epoch 92/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.2748\n",
            "Epoch 93/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 4.4064\n",
            "Epoch 94/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.1039\n",
            "Epoch 95/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.1804\n",
            "Epoch 96/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 4.2784\n",
            "Epoch 97/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.6559\n",
            "Epoch 98/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.3915\n",
            "Epoch 99/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 4.6704\n",
            "Epoch 100/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.7462\n",
            "Epoch 101/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 6.1800\n",
            "Epoch 102/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.3765\n",
            "Epoch 103/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.9390\n",
            "Epoch 104/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.1136\n",
            "Epoch 105/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.9936\n",
            "Epoch 106/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.7492\n",
            "Epoch 107/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.1555\n",
            "Epoch 108/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.6885\n",
            "Epoch 109/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5195\n",
            "Epoch 110/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.4434\n",
            "Epoch 111/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.1463\n",
            "Epoch 112/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.5011\n",
            "Epoch 113/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.2008\n",
            "Epoch 114/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.8918\n",
            "Epoch 115/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5493\n",
            "Epoch 116/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.6859\n",
            "Epoch 117/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.0350\n",
            "Epoch 118/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.2152\n",
            "Epoch 119/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0763\n",
            "Epoch 120/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 3.4612\n",
            "Epoch 121/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.4603\n",
            "Epoch 122/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 5.1715\n",
            "Epoch 123/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.5598\n",
            "Epoch 124/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.5846\n",
            "Epoch 125/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.8481\n",
            "Epoch 126/200\n",
            "2/2 [==============================] - 0s 13ms/step - loss: 1.9305\n",
            "Epoch 127/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.0554\n",
            "Epoch 128/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1290\n",
            "Epoch 129/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5990\n",
            "Epoch 130/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.7106\n",
            "Epoch 131/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.7400\n",
            "Epoch 132/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.3479\n",
            "Epoch 133/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9417\n",
            "Epoch 134/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 2.5527\n",
            "Epoch 135/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.8205\n",
            "Epoch 136/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 5.1380\n",
            "Epoch 137/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5392\n",
            "Epoch 138/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.6897\n",
            "Epoch 139/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.0260\n",
            "Epoch 140/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.1244\n",
            "Epoch 141/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5214\n",
            "Epoch 142/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.7054\n",
            "Epoch 143/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.9351\n",
            "Epoch 144/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.3793\n",
            "Epoch 145/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.5321\n",
            "Epoch 146/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6093\n",
            "Epoch 147/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.2522\n",
            "Epoch 148/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5872\n",
            "Epoch 149/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5957\n",
            "Epoch 150/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 4.4610\n",
            "Epoch 151/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 3.4003\n",
            "Epoch 152/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5128\n",
            "Epoch 153/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.3849\n",
            "Epoch 154/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6367\n",
            "Epoch 155/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4669\n",
            "Epoch 156/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.2252\n",
            "Epoch 157/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 2.7397\n",
            "Epoch 158/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.8426\n",
            "Epoch 159/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.1912\n",
            "Epoch 160/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.4288\n",
            "Epoch 161/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.4499\n",
            "Epoch 162/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.9341\n",
            "Epoch 163/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.9356\n",
            "Epoch 164/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.0786\n",
            "Epoch 165/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.4384\n",
            "Epoch 166/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.2430\n",
            "Epoch 167/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.2263\n",
            "Epoch 168/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.8407\n",
            "Epoch 169/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.2168\n",
            "Epoch 170/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.8378\n",
            "Epoch 171/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.5439\n",
            "Epoch 172/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.9278\n",
            "Epoch 173/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.9609\n",
            "Epoch 174/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.7393\n",
            "Epoch 175/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.7789\n",
            "Epoch 176/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 0.6290\n",
            "Epoch 177/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.1039\n",
            "Epoch 178/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.7942\n",
            "Epoch 179/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 0.6323\n",
            "Epoch 180/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.7762\n",
            "Epoch 181/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.1016\n",
            "Epoch 182/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 2.3043\n",
            "Epoch 183/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 2.0001\n",
            "Epoch 184/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.1375\n",
            "Epoch 185/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.4737\n",
            "Epoch 186/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.5906\n",
            "Epoch 187/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 1.2891\n",
            "Epoch 188/200\n",
            "2/2 [==============================] - 0s 10ms/step - loss: 0.2707\n",
            "Epoch 189/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 1.7544\n",
            "Epoch 190/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 0.6117\n",
            "Epoch 191/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.5755\n",
            "Epoch 192/200\n",
            "2/2 [==============================] - 0s 9ms/step - loss: 1.0328\n",
            "Epoch 193/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.7497\n",
            "Epoch 194/200\n",
            "2/2 [==============================] - 0s 8ms/step - loss: 2.5297\n",
            "Epoch 195/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 0.5619\n",
            "Epoch 196/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.1641\n",
            "Epoch 197/200\n",
            "2/2 [==============================] - 0s 7ms/step - loss: 1.8704\n",
            "Epoch 198/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 0.5277\n",
            "Epoch 199/200\n",
            "2/2 [==============================] - 0s 11ms/step - loss: 1.1215\n",
            "Epoch 200/200\n",
            "2/2 [==============================] - 0s 12ms/step - loss: 1.2805\n",
            "1/1 [==============================] - 0s 85ms/step\n",
            "Neural Network Mean Squared Error: 1.8490710198716442\n",
            "Neural Network R-squared: -6.396284079486577\n",
            "XGBoost Mean Squared Error: 3.2769364607565876\n",
            "XGBoost R-squared: -12.10774584302635\n",
            "Neural Network Predicted strikeout: 11.856968\n",
            "XGBoost Predicted strikeout: 12.2954855\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.models import Sequential\n",
        "from tensorflow.keras.layers import Dense, Dropout\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.metrics import mean_squared_error, r2_score\n",
        "import xgboost as xgb\n",
        "\n",
        "# Organize the data into a DataFrame\n",
        "data = {\n",
        "    \"innings_pitched\": [7, 7, 6, 6, 7, 5, 5, 7, 7, 6],\n",
        "    \"total_batter_faced\": [24, 24, 26, 24, 25, 21, 23, 22, 27, 24],\n",
        "    \"strikeouts_per_nine_inning\": [12.9, 11.6, 18, 13.5, 10.3, 10.8, 14.4, 12.9, 12.9, 13.5],\n",
        "    \"strikeouts\": [10, 9, 12, 9, 8, 6, 8, 10, 10, 9]\n",
        "}\n",
        "df = pd.DataFrame(data)\n",
        "\n",
        "# Define the independent variables and the dependent variable\n",
        "X = df[['innings_pitched', 'total_batter_faced', 'strikeouts_per_nine_inning']].values\n",
        "y = df['strikeouts'].values\n",
        "\n",
        "# Split the data into training and testing sets\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "# Standardize the features for the neural network\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "# Build the neural network model\n",
        "model = Sequential()\n",
        "model.add(Dense(128, input_dim=X_train_scaled.shape[1], activation='relu'))\n",
        "model.add(Dropout(0.5))\n",
        "model.add(Dense(64, activation='relu'))\n",
        "model.add(Dense(32, activation='relu'))\n",
        "model.add(Dense(1, activation='linear'))\n",
        "\n",
        "# Compile the model with a modified learning rate\n",
        "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), loss='mean_squared_error')\n",
        "\n",
        "# Train the neural network model\n",
        "model.fit(X_train_scaled, y_train, epochs=200, batch_size=4, verbose=1)\n",
        "\n",
        "# Predict on the test set with the neural network\n",
        "y_pred_nn = model.predict(X_test_scaled)\n",
        "\n",
        "# Calculate performance metrics for the neural network\n",
        "mse_nn = mean_squared_error(y_test, y_pred_nn)\n",
        "r2_nn = r2_score(y_test, y_pred_nn)\n",
        "\n",
        "print(f\"Neural Network Mean Squared Error: {mse_nn}\")\n",
        "print(f\"Neural Network R-squared: {r2_nn}\")\n",
        "\n",
        "# Prepare data for XGBoost\n",
        "dtrain = xgb.DMatrix(X_train, label=y_train)\n",
        "dtest = xgb.DMatrix(X_test, label=y_test)\n",
        "\n",
        "# Set parameters for XGBoost\n",
        "params = {\n",
        "    'objective': 'reg:squarederror',\n",
        "    'max_depth': 3,\n",
        "    'eta': 0.1,\n",
        "    'subsample': 0.7\n",
        "}\n",
        "\n",
        "# Train the XGBoost model\n",
        "xgboost_model = xgb.train(params, dtrain, num_boost_round=200)\n",
        "\n",
        "# Predict on the test set with XGBoost\n",
        "y_pred_xgb = xgboost_model.predict(dtest)\n",
        "\n",
        "# Calculate performance metrics for XGBoost\n",
        "mse_xgb = mean_squared_error(y_test, y_pred_xgb)\n",
        "r2_xgb = r2_score(y_test, y_pred_xgb)\n",
        "\n",
        "print(f\"XGBoost Mean Squared Error: {mse_xgb}\")\n",
        "print(f\"XGBoost R-squared: {r2_xgb}\")\n",
        "\n",
        "# Output the first model prediction\n",
        "print(\"Neural Network Predicted strikeout:\", y_pred_nn.flatten()[0])\n",
        "print(\"XGBoost Predicted strikeout:\", y_pred_xgb[0])\n"
      ]
    }
  ]
}